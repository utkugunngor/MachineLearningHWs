# -*- coding: utf-8 -*-
"""Homework1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DFqjo6FW7WWnDE8cgmRlOJ7xL-_M6LMP

"""

# Import the required modules
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as T
from torchvision.datasets import MNIST
from torch.utils.data import random_split, DataLoader
from tqdm import tqdm
from torchvision.datasets import CIFAR10
import torchvision.transforms as T


seed = 1234
torch.manual_seed(seed)

train_transform = T. Compose ([
# can add additional transforms on images
T. ToTensor () , # convert images to PyTorch tensors
T. Grayscale () , # RGB to grayscale
T. Normalize ( mean =(0.5 ,) , std=(0.5 ,) ) # n or ma li za ti on
# speeds up the convergence
# and improves the accuracy
])
val_transform = test_transform = T. Compose ([
T. ToTensor () ,
T. Grayscale () ,
T. Normalize ( mean =(0.5 ,) , std=(0.5 ,) )
])
train_set = CIFAR10 ( root ='CIFAR10', train =True ,
transform = train_transform , download = True )
test_set = CIFAR10 ( root ='CIFAR10', train =False ,
transform = test_transform , download = True )

# Fix the randomness
seed = 1234
torch.manual_seed(seed)

# Download the dataset, and split it into Train, Val, and Test sets.
transform = T.Compose([T.ToTensor(), T.Normalize(mean=(0.5,), std=(0.5,))])

train_set_length = int(0.8 * len(train_set))
val_set_length = len(train_set) - train_set_length

train_set, val_set = random_split(train_set, [train_set_length, val_set_length])

# Define the data loaders
batch_size = 32
train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_set, batch_size=batch_size)
test_loader = DataLoader(test_set, batch_size=batch_size)

"""<center><img src=https://machinelearningmastery.com/wp-content/uploads/2019/02/Plot-of-a-Subset-of-Images-from-the-MNIST-Dataset.png width="400">"""

# Define the ANN
for numLayers in range(1,4):
  for actFunction in range(1,4):
    for lRate in range(2,6):
      
      class MyModel(nn.Module):
          def __init__(self, num_layer1, num_layer2):
              super().__init__()
              self.layer1 = nn.Linear(in_features=32*32, out_features=num_layer1)
              if(numLayers > 1): self.layer2 = nn.Linear(in_features=num_layer1, out_features=num_layer2)
              if(numLayers > 2): self.layer3 = nn.Linear(in_features=num_layer2, out_features=10)
          if(actFunction == 1):
            def forward(self, x):
              x = torch.flatten(x, 1)
              x = F.hardswish(self.layer1(x))
              if(numLayers > 1): x = F.hardswish(self.layer2(x))
              if(numLayers > 2): x = self.layer3(x)
              # not have to add softmax layer here
              return x
          elif(actFunction == 2):
            def forward(self, x):
              x = torch.flatten(x, 1)
              x = F.sigmoid(self.layer1(x))
              if(numLayers > 1): x = F.sigmoid(self.layer2(x))
              if(numLayers > 2): x = self.layer3(x)
              # not have to add softmax layer here
              return x
          else:
            def forward(self, x):
              x = torch.flatten(x, 1)
              x = F.relu(self.layer1(x))
              if(numLayers > 1): x = F.relu(self.layer2(x))
              if(numLayers > 2): x = self.layer3(x)
              # not have to add softmax layer here
              return x
      # Instantiate the model and Train it for 3 epochs
      device = 'cuda' if torch.cuda.is_available() else 'cpu'

      model = MyModel(200, 150).to(device)
      loss_function = nn.CrossEntropyLoss()
      optimizer = torch.optim.Adam(model.parameters(), lr=pow(10, -1*lRate))
      torch.optim.Adam

      num_epochs = 20
      for epoch in tqdm(range(num_epochs)):
          
          # Training
          model.train()
          accum_train_loss = 0
          for i, (imgs, labels) in enumerate(train_loader, start=1):
              imgs, labels = imgs.to(device), labels.to(device)
              output = model(imgs)
              loss = loss_function(output, labels)

              # accumlate the loss
              accum_train_loss += loss.item()

              # backpropagation
              optimizer.zero_grad()
              loss.backward()
              optimizer.step()
          
          # Validation
          model.eval()
          accum_val_loss = 0
          with torch.no_grad():
              for j, (imgs, labels) in enumerate(val_loader, start=1):
                  imgs, labels = imgs.to(device), labels.to(device)
                  output = model(imgs)
                  accum_val_loss += loss_function(output, labels).item()

          # print statistics of the epoch
          print(f'Epoch = {epoch} | Train Loss = {accum_train_loss / i:.4f}\tVal Loss = {accum_val_loss / j:.4f}')
          

      # Compute Test Accuracy
      model.eval()
      with torch.no_grad():
          correct = total = 0
          for images, labels in test_loader:
              images, labels = images.to(device), labels.to(device)
              output = model(images)
              
              _, predicted_labels = torch.max(output, 1)
              correct += (predicted_labels == labels).sum()
              total += labels.size(0)
      # Compute Val Accuracy
      model.eval()
      with torch.no_grad():
          correct2 = total2 = 0
          for images, labels in val_loader:
              images, labels = images.to(device), labels.to(device)
              output2 = model(images)
              
              _, predicted_labels2 = torch.max(output2, 1)
              correct2 += (predicted_labels2 == labels).sum()
              total2 += labels.size(0)

      print(f'Test Accuracy = {100 * correct/total :.3f}%')
      print(f'Val Accuracy = {100 * correct2/total2 :.3f}%')
      print(numLayers, actFunction, lRate)
      print("------------------------------------------------")
      # logits --> unnormalized probabilites
      #nn.BatchNorm1d
      #nn.Dropout





